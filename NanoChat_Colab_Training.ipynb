{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoChat Training on Google Colab ðŸš€\n",
    "\n",
    "This notebook runs NanoChat training on Google Colab's A100 GPU.\n",
    "\n",
    "## âš ï¸ Important Notes\n",
    "- **Limited training** for demo/learning purposes\n",
    "- Full training would take days/weeks\n",
    "- Model quality will be lower than production\n",
    "- Colab has time limits (12-24 hours max)\n",
    "\n",
    "## Setup Steps\n",
    "1. Change runtime: Runtime â†’ Change runtime type â†’ GPU â†’ A100\n",
    "2. Run all cells sequentially\n",
    "3. Choose your config (demo/small/medium)\n",
    "\n",
    "**Recommended**: Start with `demo` config first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  No GPU detected! Please change runtime to GPU.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv (package manager)\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "import os\n",
    "os.environ['PATH'] = os.path.expanduser('~/.cargo/bin') + ':' + os.environ.get('PATH', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone nanochat repository\n",
    "!git clone https://github.com/ideaweaver-ai/nanochat.git\n",
    "%cd nanochat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Python environment\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['NANOCHAT_BASE_DIR'] = '/content/.cache/nanochat'\n",
    "\n",
    "# Create venv and install dependencies\n",
    "!uv venv\n",
    "!uv sync --extra gpu\n",
    "\n",
    "# Activate venv (for subsequent cells)\n",
    "import sys\n",
    "sys.path.insert(0, '/content/nanochat/.venv/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Choose Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your config: 'demo', 'small', or 'medium'\n",
    "CONFIG = 'demo'  # Change this: demo|small|medium\n",
    "\n",
    "# Configuration settings\n",
    "if CONFIG == 'demo':\n",
    "    DEPTH = 8\n",
    "    MAX_SEQ_LEN = 512\n",
    "    DEVICE_BATCH_SIZE = 4\n",
    "    TOTAL_BATCH_SIZE = 8192\n",
    "    BASE_ITERATIONS = 500\n",
    "    MID_ITERATIONS = 200\n",
    "    SFT_ITERATIONS = 100\n",
    "    DATA_SHARDS = 10\n",
    "    print(\"âœ… DEMO MODE: ~1 hour total training\")\n",
    "elif CONFIG == 'small':\n",
    "    DEPTH = 10\n",
    "    MAX_SEQ_LEN = 1024\n",
    "    DEVICE_BATCH_SIZE = 4\n",
    "    TOTAL_BATCH_SIZE = 16384\n",
    "    BASE_ITERATIONS = 2000\n",
    "    MID_ITERATIONS = 500\n",
    "    SFT_ITERATIONS = 200\n",
    "    DATA_SHARDS = 20\n",
    "    print(\"âœ… SMALL MODE: ~3-6 hours total training\")\n",
    "elif CONFIG == 'medium':\n",
    "    DEPTH = 12\n",
    "    MAX_SEQ_LEN = 1024\n",
    "    DEVICE_BATCH_SIZE = 4\n",
    "    TOTAL_BATCH_SIZE = 32768\n",
    "    BASE_ITERATIONS = 5000\n",
    "    MID_ITERATIONS = 1000\n",
    "    SFT_ITERATIONS = 500\n",
    "    DATA_SHARDS = 40\n",
    "    print(\"âœ… MEDIUM MODE: ~9-13 hours total training\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown config: {CONFIG}\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Depth: {DEPTH}\")\n",
    "print(f\"  Max seq len: {MAX_SEQ_LEN}\")\n",
    "print(f\"  Batch size: {DEVICE_BATCH_SIZE}\")\n",
    "print(f\"  Base iterations: {BASE_ITERATIONS}\")\n",
    "print(f\"  Mid iterations: {MID_ITERATIONS}\")\n",
    "print(f\"  SFT iterations: {SFT_ITERATIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Rust (needed for tokenizer)\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "import os\n",
    "os.environ['PATH'] = os.path.expanduser('~/.cargo/bin') + ':' + os.environ.get('PATH', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Rust tokenizer\n",
    "!uv run maturin develop --release --manifest-path rustbpe/Cargo.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize report\n",
    "!python -m nanochat.report reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download minimal data for tokenizer\n",
    "!python -m nanochat.dataset -n 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data (in background)\n",
    "import subprocess\n",
    "import os\n",
    "os.environ['NANOCHAT_BASE_DIR'] = '/content/.cache/nanochat'\n",
    "download_process = subprocess.Popen(\n",
    "    ['python', '-m', 'nanochat.dataset', '-n', str(DATA_SHARDS)],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "print(f\"Started downloading {DATA_SHARDS} data shards in background...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizer\n",
    "!python -m scripts.tok_train --max_chars=500000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tokenizer\n",
    "!python -m scripts.tok_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Stage 1 - Base Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for data download to complete\n",
    "download_process.wait()\n",
    "print(\"âœ… Data download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base training with limited iterations\n",
    "print(f\"Starting BASE TRAINING ({BASE_ITERATIONS} iterations)...\")\n",
    "print(f\"This will take approximately {BASE_ITERATIONS // 100}-{BASE_ITERATIONS // 50} minutes\")\n",
    "\n",
    "!python -m scripts.base_train \\\n",
    "    --depth={DEPTH} \\\n",
    "    --max_seq_len={MAX_SEQ_LEN} \\\n",
    "    --device_batch_size={DEVICE_BATCH_SIZE} \\\n",
    "    --total_batch_size={TOTAL_BATCH_SIZE} \\\n",
    "    --num_iterations={BASE_ITERATIONS} \\\n",
    "    --target_param_data_ratio=-1 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=8192 \\\n",
    "    --core_metric_every=-1 \\\n",
    "    --sample_every=200 \\\n",
    "    --run=dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "!python -m scripts.base_loss --device_batch_size={DEVICE_BATCH_SIZE}\n",
    "!python -m scripts.base_eval --max-per-task=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stage 2 - Midtraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download identity conversations\n",
    "!curl -L -o /content/.cache/nanochat/identity_conversations.jsonl https://karpathy-public.s3.us-west-2.amazonaws.com/identity_conversations.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Midtraining with limited iterations\n",
    "print(f\"Starting MIDTRAINING ({MID_ITERATIONS} iterations)...\")\n",
    "print(f\"This will take approximately {MID_ITERATIONS // 50}-{MID_ITERATIONS // 25} minutes\")\n",
    "\n",
    "!python -m scripts.mid_train \\\n",
    "    --device_batch_size={DEVICE_BATCH_SIZE} \\\n",
    "    --max_seq_len={MAX_SEQ_LEN} \\\n",
    "    --total_batch_size={TOTAL_BATCH_SIZE} \\\n",
    "    --num_iterations={MID_ITERATIONS} \\\n",
    "    --eval_every=50 \\\n",
    "    --eval_tokens=8192 \\\n",
    "    --run=dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate midtrained model\n",
    "!python -m scripts.chat_eval -- -i mid --max-problems=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Stage 3 - Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SFT batch size\n",
    "SFT_BATCH_SIZE = max(DEVICE_BATCH_SIZE // 2, 2)\n",
    "print(f\"SFT batch size: {SFT_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT with limited iterations\n",
    "print(f\"Starting SFT ({SFT_ITERATIONS} iterations)...\")\n",
    "print(f\"This will take approximately {SFT_ITERATIONS // 20}-{SFT_ITERATIONS // 10} minutes\")\n",
    "\n",
    "!python -m scripts.chat_sft \\\n",
    "    --device_batch_size={SFT_BATCH_SIZE} \\\n",
    "    --num_iterations={SFT_ITERATIONS} \\\n",
    "    --eval_every=25 \\\n",
    "    --eval_steps=10 \\\n",
    "    --eval_metrics_max_problems=50 \\\n",
    "    --run=dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SFT model\n",
    "!python -m scripts.chat_eval -- -i sft --max-problems=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "!python -m nanochat.report generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Chat with Your Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start web chat interface\n",
    "# Note: This will start a server - you can access it via the URL shown\n",
    "# For Colab, you may need to use ngrok or similar for external access\n",
    "print(\"To chat with your model, run:\")\n",
    "print(\"  python -m scripts.chat_web\")\n",
    "print(\"\\nOr use CLI:\")\n",
    "print(\"  python -m scripts.chat_cli -p 'Your question here'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Save checkpoints and reports\n",
    "!mkdir -p /content/drive/MyDrive/nanochat_outputs\n",
    "!cp -r /content/.cache/nanochat/* /content/drive/MyDrive/nanochat_outputs/ 2>/dev/null || true\n",
    "!cp report.md /content/drive/MyDrive/nanochat_outputs/ 2>/dev/null || true\n",
    "print(\"âœ… Saved to Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
