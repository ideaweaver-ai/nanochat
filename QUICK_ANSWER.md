# Quick Answer: Does run_single_a100.sh Handle All Stages?

## âœ… YES - The Script Handles All Three Stages

The `run_single_a100.sh` script executes all three training stages sequentially:

### Stage 1: Base Training âœ…
**Lines 126-148 in run_single_a100.sh**
```bash
python -m scripts.base_train \
    --depth=$DEPTH \
    --max_seq_len=$MAX_SEQ_LEN \
    --device_batch_size=$DEVICE_BATCH_SIZE \
    --total_batch_size=$TOTAL_BATCH_SIZE \
    --run=$WANDB_RUN
```

**Dataset**: FineWeb-Edu 100BT (raw text, parquet shards)
- Downloads automatically via `python -m nanochat.dataset -n $DATA_SHARDS`
- ~80-180 shards depending on model size
- Each shard: ~250M characters

**Features**:
- âœ… Pretraining on raw text
- âœ… Chinchilla scaling (tokens = 20 Ã— params)
- âœ… Muon optimizer (transformer layers)
- âœ… AdamW optimizer (embeddings + LM head)
- âœ… Gradient accumulation (automatic)
- âœ… LR warmup/warmdown
- âœ… Checkpointing support
- âœ… CORE metric evaluation

---

### Stage 2: Midtraining âœ…
**Lines 150-163 in run_single_a100.sh**
```bash
python -m scripts.mid_train \
    --device_batch_size=$DEVICE_BATCH_SIZE \
    --max_seq_len=$MAX_SEQ_LEN \
    --total_batch_size=$TOTAL_BATCH_SIZE \
    --run=$WANDB_RUN
```

**Dataset**: TaskMixture (~850K examples)
- SmolTalk: 460K conversations
- MMLU: 100K multiple choice
- GSM8K: 8K math problems
- Identity: 2K synthetic conversations
- SimpleSpelling: 200K spelling tasks
- SpellingBee: 80K counting tasks

**Features**:
- âœ… Conversation format learning
- âœ… Tool use (Python calculator)
- âœ… Multiple choice reasoning
- âœ… Same optimizer setup (Muon + AdamW)
- âœ… Progress-based LR scheduling

---

### Stage 3: Supervised Fine-Tuning (SFT) âœ…
**Lines 165-177 in run_single_a100.sh**
```bash
python -m scripts.chat_sft \
    --device_batch_size=$SFT_BATCH_SIZE \
    --run=$WANDB_RUN
```

**Dataset**: TaskMixture (~23K examples)
- ARC-Easy: 2.3K science questions
- ARC-Challenge: 1.1K hard science
- GSM8K: 8K math problems
- SmolTalk: 10K conversations (subset)
- Identity: 1K conversations
- SimpleSpelling: 300 examples
- SpellingBee: 300 examples

**Features**:
- âœ… Domain adaptation for chat
- âœ… Variable-length sequences
- âœ… Masked loss (only assistant tokens)
- âœ… Smaller, focused dataset

---

## Training Datasets Summary

| Stage | Primary Dataset | Size | Source |
|-------|----------------|------|--------|
| **1. Base** | FineWeb-Edu 100BT | ~4-11B tokens | HuggingFace (karpathy/fineweb-edu-100b-shuffle) |
| **2. Mid** | TaskMixture | ~850K examples | Multiple (SmolTalk, MMLU, GSM8K, etc.) |
| **3. SFT** | TaskMixture | ~23K examples | Curated subset of Stage 2 datasets |

---

## All Features Are Preserved

The script maintains all features from the original 8xH100 setup:
- âœ… All three training stages
- âœ… All optimizer features (Muon + AdamW)
- âœ… All evaluation steps
- âœ… All dataset downloads
- âœ… All checkpointing
- âœ… All reporting

**Only difference**: Runs on single GPU instead of 8 GPUs (slower, but functionally identical)

---

## To Run

Simply execute:
```bash
bash run_single_a100.sh
```

Or with specific config:
```bash
CONFIG=conservative bash run_single_a100.sh  # For 40GB A100
CONFIG=aggressive bash run_single_a100.sh   # For 80GB A100
```

The script handles everything automatically! ðŸš€
